import os
import uuid
import json
import glob
import time
import requests  # C·∫ßn th√™m th∆∞ vi·ªán n√†y
from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from sqlalchemy import create_engine, text

# Import LangChain
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings  # Import base class
from langchain_postgres import PGVector

# --- 1. C·∫§U H√åNH ---
load_dotenv()

# ‚ö†Ô∏è QUAN TR·ªåNG: Thay link Ngrok c·ªßa b·∫°n v√†o ƒë√¢y
# Link ph·∫£i c√≥ d·∫°ng: https://xxxx-xxxx.ngrok-free.app/embed_batch
COLAB_API_URL = "https://domelike-ora-gorgedly.ngrok-free.dev/embed_batch"

DB_CONNECTION = os.getenv("CONNECTION_STRING")
DB_COLLECTION_NAME = os.getenv("COLLECTION_NAME")
DATA_DIR = "investopedia_terms"

if not DB_CONNECTION or not DB_COLLECTION_NAME:
    raise ValueError("‚ùå L·ªói c·∫•u h√¨nh: Ki·ªÉm tra file .env")

engine = create_engine(DB_CONNECTION)


# --- 2. ƒê·ªäNH NGHƒ®A CUSTOM EMBEDDING CLASS ---
class ColabEmbeddings(Embeddings):
    """
    Class n√†y gi√∫p LangChain n√≥i chuy·ªán v·ªõi API Colab c·ªßa b·∫°n.
    N√≥ ƒë√≥ng vai tr√≤ thay th·∫ø cho GoogleGenerativeAIEmbeddings.
    """

    def __init__(self, api_url: str):
        self.api_url = api_url

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """H√†m n√†y ƒë∆∞·ª£c PGVector g·ªçi ƒë·ªÉ embed m·ªôt danh s√°ch vƒÉn b·∫£n."""
        try:
            # Th√™m timeout=120 (ch·ªù t·ªëi ƒëa 2 ph√∫t) ƒë·ªÉ an to√†n cho batch l·ªõn
            response = requests.post(self.api_url, json={"texts": texts}, timeout=120)

            if response.status_code == 200:
                data = response.json()
                return data['embeddings']
            else:
                # In ra l·ªói chi ti·∫øt n·∫øu Server tr·∫£ v·ªÅ 500 ho·∫∑c 422
                print(f"‚ö†Ô∏è Server Response: {response.text}")
                raise ValueError(f"API Error {response.status_code}")
        except Exception as e:
            print(f"‚ùå L·ªói khi g·ªçi API Colab: {e}")
            raise e

    def embed_query(self, text: str) -> List[float]:
        """H√†m n√†y ƒë∆∞·ª£c d√πng khi b·∫°n Search (Retrieval)."""
        # T·∫≠n d·ª•ng lu√¥n h√†m embed_documents cho ti·ªán
        return self.embed_documents([text])[0]


def init_db():
    """T·∫°o b·∫£ng 'doc_parents' n·∫øu ch∆∞a c√≥."""
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS doc_parents (
        parent_id TEXT PRIMARY KEY,
        content TEXT,
        metadata JSONB,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    with engine.connect() as conn:
        conn.execute(text(create_table_sql))
        conn.commit()
    print("‚úÖ ƒê√£ ki·ªÉm tra/t·∫°o b·∫£ng 'doc_parents'.")


def clean_text(text: str) -> str:
    lines = text.split('\n')
    unique_lines = []
    prev_line = ""
    for line in lines:
        stripped = line.strip()
        if stripped and stripped != prev_line:
            unique_lines.append(line)
            prev_line = stripped
        elif stripped == "":
            unique_lines.append(line)
    return "\n".join(unique_lines)


# --- 3. X·ª¨ L√ù D·ªÆ LI·ªÜU ---
def process_document_hybrid(raw_text: str, source_filename: str) -> Tuple[List[Dict], List[Document]]:
    cleaned_text = clean_text(raw_text)

    # A. Parent Chunking
    headers_to_split_on = [("#", "Header 1"), ("##", "Header 2"), ("###", "Header 3")]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
    parent_docs = markdown_splitter.split_text(cleaned_text)

    # B. Child Chunking
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # TƒÉng l√™n x√≠u v√¨ BGE-M3 ch·ªãu ƒë∆∞·ª£c context d√†i
        chunk_overlap=150,
        separators=["\n\n", "\n", ". ", " ", ""]
    )

    parents_data = []
    child_docs = []

    for i, parent in enumerate(parent_docs):
        parent_id = str(uuid.uuid4())

        # Chu·∫©n h√≥a metadata
        clean_metadata = {}
        for key, value in parent.metadata.items():
            clean_key = key.replace(" ", "_").lower()
            clean_metadata[clean_key] = value
        clean_metadata["source"] = source_filename

        parents_data.append({
            "parent_id": parent_id,
            "content": parent.page_content,
            "metadata": json.dumps(clean_metadata)
        })

        child_chunks = text_splitter.split_text(parent.page_content)

        for j, chunk_text in enumerate(child_chunks):
            child_metadata = {
                "chunk_id": str(uuid.uuid4()),
                "parent_id": parent_id,
                "chunk_index": j,
                "level": "child",
                "source": source_filename
            }
            child_metadata.update(clean_metadata)

            doc = Document(page_content=chunk_text, metadata=child_metadata)
            child_docs.append(doc)

    return parents_data, child_docs


def load_and_process_folder(folder_path: str):
    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c '{folder_path}' ch∆∞a t·ªìn t·∫°i.")
        return [], []

    md_files = glob.glob(os.path.join(folder_path, "*.md"))
    if not md_files:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file .md n√†o.")
        return [], []

    all_parents = []
    all_children = []
    print(f"üìÇ T√¨m th·∫•y {len(md_files)} file Markdown.")

    for file_path in md_files:
        filename = os.path.basename(file_path)
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                raw_text = f.read()
            parents, children = process_document_hybrid(raw_text, filename)
            all_parents.extend(parents)
            all_children.extend(children)
        except Exception as e:
            print(f"   ‚ùå L·ªói ƒë·ªçc file {filename}: {e}")

    return all_parents, all_children


# --- 4. H√ÄM L∆ØU (S·ª¨ D·ª§NG CUSTOM API) ---
def save_hybrid_data(parents_data: List[Dict], child_docs: List[Document]):
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu l∆∞u d·ªØ li·ªáu...")

    # B∆Ø·ªöC 1: L∆∞u Parents (SQL)
    BATCH_SIZE_SQL = 1000
    if parents_data:
        total_parents = len(parents_data)
        print(f"üíæ ƒêang l∆∞u {total_parents} Parents v√†o SQL...")

        insert_query = text("""
            INSERT INTO doc_parents (parent_id, content, metadata)
            VALUES (:parent_id, :content, :metadata)
            ON CONFLICT (parent_id) DO NOTHING;
        """)

        with engine.connect() as conn:
            for i in range(0, total_parents, BATCH_SIZE_SQL):
                batch = parents_data[i: i + BATCH_SIZE_SQL]
                conn.execute(insert_query, batch)
                conn.commit()
                print(f"   ‚úÖ SQL Batch {i} -> {min(i + BATCH_SIZE_SQL, total_parents)}")

    # B∆Ø·ªöC 2: L∆∞u Children (Vector Store qua API Colab)
    # V√¨ API Colab x·ª≠ l√Ω r·∫•t nhanh, ta c√≥ th·ªÉ tƒÉng batch size l√™n 
    BATCH_SIZE_VECTOR = 1000

    if child_docs:
        total_children = len(child_docs)
        print(f"\nüíæ ƒêang embed v√† l∆∞u {total_children} Children qua API Colab...")

        # --- KH·ªûI T·∫†O CUSTOM EMBEDDING ---
        embeddings_model = ColabEmbeddings(api_url=COLAB_API_URL)

        try:
            # Kh·ªüi t·∫°o Vector Store
            # L∆∞u √Ω: PGVector s·∫Ω t·ª± ƒë·ªông g·ªçi embeddings_model.embed_documents()
            vector_store = PGVector(
                embeddings=embeddings_model,
                collection_name=DB_COLLECTION_NAME,
                connection=DB_CONNECTION,
                use_jsonb=True,
            )

            # Chia nh·ªè ƒë·ªÉ g·ª≠i API
            for i in range(0, total_children, BATCH_SIZE_VECTOR):
                batch = child_docs[i: i + BATCH_SIZE_VECTOR]
                try:
                    # D√≤ng n√†y s·∫Ω k√≠ch ho·∫°t ColabEmbeddings.embed_documents
                    # N√≥ g·ª≠i 50 c√¢u l√™n Colab -> Colab tr·∫£ v·ªÅ 50 vector -> L∆∞u v√†o DB
                    vector_store.add_documents(batch)

                    percent = ((i + len(batch)) / total_children) * 100
                    print(f"   ‚úÖ Vector Batch {i} -> {min(i + BATCH_SIZE_VECTOR, total_children)} ({percent:.1f}%)")

                except Exception as e:
                    print(f"   ‚ùå L·ªói Batch {i}: {e}")
                    time.sleep(2)  # Ngh·ªâ x√≠u r·ªìi ch·∫°y ti·∫øp

            print("‚úÖ ƒê√£ l∆∞u TO√ÄN B·ªò Vectors th√†nh c√¥ng!")

        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o Vector Store: {e}")


# --- MAIN ---
if __name__ == "__main__":
    init_db()

    # Ki·ªÉm tra URL tr∆∞·ªõc khi ch·∫°y
    if "ngrok-free.dev" not in COLAB_API_URL:
        print("‚õî L·ªñI: B·∫°n ch∆∞a d√°n link Ngrok v√†o bi·∫øn COLAB_API_URL!")
    else:
        final_parents, final_children = load_and_process_folder(DATA_DIR)
        if final_parents and final_children:
            save_hybrid_data(final_parents, final_children)
            print("\nüéâ HO√ÄN T·∫§T!")
        else:
            print("\n‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu.")