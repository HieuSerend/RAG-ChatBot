import os
import uuid
import json
import time
import requests
import fitz  # PyMuPDF
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_postgres import PGVector

# --- 1. Cáº¤U HÃŒNH ---
load_dotenv()

# TÃªn file PDF cá»§a báº¡n
INPUT_PDF_PATH = "glossary.pdf" 
START_PAGE = 12  # Index trang (Trang 11 lÃ  index 10)
END_PAGE = 603  

# Cáº¥u hÃ¬nh cá»©ng Source theo yÃªu cáº§u
FIXED_SOURCE_NAME = "OECD Glossary of Statistical Terms"

COLAB_API_URL = os.getenv("COLAB_API_URL", "https://unapprovable-bryon-subpeltately.ngrok-free.dev/embed_batch")
DB_CONNECTION = os.getenv("CONNECTION_STRING", "postgresql+psycopg://postgres:password@localhost:5433/rag_chatbot")
DB_COLLECTION_NAME = os.getenv("COLLECTION_NAME", "gemini_knowledge_base")

engine = create_engine(DB_CONNECTION)

# --- 2. EMBEDDING CLASS (GIá»® NGUYÃŠN) ---
class ColabEmbeddings(Embeddings):
    def __init__(self, api_url: str):
        self.api_url = api_url

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            # Timeout cao hÆ¡n cho an toÃ n
            response = requests.post(self.api_url, json={"texts": texts}, timeout=120)
            if response.status_code == 200:
                return response.json()['embeddings']
            else:
                print(f"âš ï¸ API Error: {response.text}")
                return []
        except Exception as e:
            print(f"âŒ Lá»—i API: {e}")
            return []

    def embed_query(self, text: str) -> List[float]:
        return self.embed_documents([text])[0]

def init_db():
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS doc_parents (
        parent_id TEXT PRIMARY KEY,
        content TEXT,
        metadata JSONB,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    with engine.connect() as conn:
        conn.execute(text(create_table_sql))
        conn.commit()
    print("âœ… ÄÃ£ káº¿t ná»‘i Database.")

# --- 3. Xá»¬ LÃ PDF (Tá»I GIáº¢N) ---
# ... (Giá»¯ nguyÃªn import vÃ  config DB/API) ...

# --- Cáº¤U HÃŒNH LOGIC NHáº¬N DIá»†N ---
# Báº¡n hÃ£y Ä‘iá»n con sá»‘ báº¡n soi Ä‘Æ°á»£c á»Ÿ BÆ°á»›c 1 vÃ o Ä‘Ã¢y.
# VÃ­ dá»¥: Chá»¯ thÆ°á»ng size 9, Chá»¯ Term size 11 -> ThÃ¬ Ä‘áº·t ngÆ°á»¡ng lÃ  10.0
TERM_THRESHOLD_SIZE = 8.0 

def is_real_term(span) -> bool:
    """
    Logic má»›i:
    1. Check Size: Pháº£i lá»›n hÆ¡n ngÆ°á»¡ng quy Ä‘á»‹nh.
    2. Check Font: Váº«n nÃªn check Bold Ä‘á»ƒ cháº¯c cháº¯n (hoáº·c bá» náº¿u PDF nÃ y Term khÃ´ng bold).
    3. Check Header RÃ¡c: Loáº¡i bá» cÃ¡c chá»¯ cÃ¡i cÃ¡i to Ä‘Ã¹ng (A, B, C...) Ä‘áº§u má»¥c lá»¥c.
    """
    text = span["text"].strip()
    size = span["size"]
    font_name = span["font"].lower()
    
    # 1. Äiá»u kiá»‡n tiÃªn quyáº¿t: SIZE PHáº¢I TO
    if size <= TERM_THRESHOLD_SIZE:
        return False
        
    # 2. Loáº¡i bá» Header Má»¥c lá»¥c (Chá»¯ A, B, C... to Ä‘Ã¹ng Ä‘á»©ng má»™t mÃ¬nh)
    # ThÆ°á»ng máº¥y chá»¯ cÃ¡i Ä‘áº§u má»¥c lá»¥c size ráº¥t to (vÃ­ dá»¥ > 20)
    if size > 20:
        return False
    if len(text) == 1 and text.isupper(): # Bá» qua chá»¯ cÃ¡i Ä‘Æ¡n láº» kiá»ƒu "A", "B"
        return False

    # 3. (TÃ¹y chá»n) Váº«n check Bold cho cháº¯c Äƒn, trÃ¡nh trÆ°á»ng há»£p text thÆ°á»ng bá»‹ lá»—i font size
    is_bold = "bold" in font_name or (span["flags"] & 16)
    if not is_bold:
        return False

    # Náº¿u thá»a mÃ£n size to + bold -> LÃ  Term xá»‹n
    return True

def parse_pdf_data(pdf_path: str, start: int, end: int) -> List[Dict]:
    doc = fitz.open(pdf_path)
    extracted_data = []
    
    current_term = None
    current_def_parts = []
    
    print(f"ğŸ“– Äang quÃ©t PDF (Theo Size > {TERM_THRESHOLD_SIZE}) tá»« trang {start+1} -> {end}...")
    end = min(end, len(doc))
    
    for page_num in range(start, end):
        page = doc[page_num]
        blocks = page.get_text("dict", sort=True)["blocks"]

        for block in blocks:
            if "lines" not in block: continue
            
            for line in block["lines"]:
                # Gom text dÃ²ng Ä‘á»ƒ xá»­ lÃ½ (trÃ¡nh PDF tÃ¡ch kÃ½ tá»±)
                line_text = " ".join([s["text"] for s in line["spans"]]).strip()
                
                for span in line["spans"]:
                    text = span["text"].strip()
                    if not text: continue
                    
                    # --- LOGIC Má»šI ---
                    if is_real_term(span):
                        # 1. LÆ°u Term cÅ©
                        if current_term:
                            full_def = " ".join(current_def_parts).strip()
                            if full_def:
                                extracted_data.append({
                                    "term": current_term,
                                    "definition": full_def
                                })
                        
                        # 2. Báº¯t Ä‘áº§u Term má»›i
                        current_term = text
                        current_def_parts = []
                    
                    else:
                        # Ná»™i dung Definition (Bao gá»“m cáº£ Source, Context... vÃ¬ chÃºng size nhá»)
                        # Code nÃ y sáº½ gom háº¿t "Source: ABC" vÃ o lÃ m má»™t pháº§n cá»§a definition luÃ´n
                        # Ä‘Ãºng nhÆ° Ã½ báº¡n muá»‘n "term: ..., definition: ... (kÃ¨m source)"
                        if current_term:
                            current_def_parts.append(text)
        
        if (page_num + 1) % 50 == 0:
            print(f"   -> Xong trang {page_num + 1}")

    if current_term and current_def_parts:
        extracted_data.append({
            "term": current_term,
            "definition": " ".join(current_def_parts).strip()
        })
        
    return extracted_data

def prepare_documents(raw_data: List[Dict]) -> Tuple[List[Dict], List[Document]]:
    """
    Format dá»¯ liá»‡u theo yÃªu cáº§u: "term: ..., definition: ..."
    """
    # Splitter chá»‰ dÃ¹ng náº¿u 1 definition quÃ¡ dÃ i (vÆ°á»£t quÃ¡ context window)
    # Náº¿u khÃ´ng muá»‘n cáº¯t, cÃ³ thá»ƒ set chunk_size tháº­t lá»›n (vd: 2000)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    parents_data = []
    child_docs = []

    for item in raw_data:
        term = item["term"]
        definition = item["definition"]
        
        # 1. FORMAT CHUá»–I CONTENT DUY NHáº¤T
        formatted_content = f"term: {term}, definition: {definition}"
        
        # 2. METADATA Tá»I GIáº¢N
        metadata = {
            "source": FIXED_SOURCE_NAME
        }

        # Parent ID
        parent_id = str(uuid.uuid4())

        # Táº¡o Parent Data (SQL)
        parents_data.append({
            "parent_id": parent_id,
            "content": formatted_content,
            "metadata": json.dumps(metadata)
        })

        # Táº¡o Child Documents (Vector)
        # Náº¿u Ä‘oáº¡n text ngáº¯n, splitter sáº½ giá»¯ nguyÃªn cáº£ cá»¥m "term: ..., definition: ..."
        chunks = text_splitter.split_text(formatted_content)
        
        for i, chunk_text in enumerate(chunks):
            chunk_meta = metadata.copy()
            chunk_meta.update({
                "parent_id": parent_id,
                "chunk_id": str(uuid.uuid4())
            })
            
            # LÆ°u Ã½: chunk_text á»Ÿ Ä‘Ã¢y Ä‘Ã£ mang Ä‘á»‹nh dáº¡ng "term: ..., definition: ..."
            # trá»« khi definition quÃ¡ dÃ i bá»‹ cáº¯t Ä‘Ã´i, pháº§n sau sáº½ chá»‰ cÃ²n text definition
            # NhÆ°ng vá»›i chunk_size=1000 thÃ¬ háº§u háº¿t glossary sáº½ náº±m trá»n trong 1 chunk.
            doc = Document(page_content=chunk_text, metadata=chunk_meta)
            child_docs.append(doc)

    return parents_data, child_docs

# --- 4. LÆ¯U VÃ€O DB ---
def save_to_db(parents_data, child_docs):
    if not parents_data: return

    print(f"\nğŸš€ Äang lÆ°u {len(parents_data)} thuáº­t ngá»¯...")

    # 1. LÆ°u SQL
    with engine.connect() as conn:
        stmt = text("""
            INSERT INTO doc_parents (parent_id, content, metadata)
            VALUES (:parent_id, :content, :metadata)
            ON CONFLICT (parent_id) DO NOTHING;
        """)
        # Batch insert SQL
        for i in range(0, len(parents_data), 2000):
            conn.execute(stmt, parents_data[i:i+2000])
            conn.commit()
    print("âœ… ÄÃ£ lÆ°u parent data.")

    # 2. LÆ°u Vector
    embeddings = ColabEmbeddings(api_url=COLAB_API_URL)
    vector_store = PGVector(
        embeddings=embeddings,
        collection_name=DB_COLLECTION_NAME,
        connection=DB_CONNECTION,
        use_jsonb=True,
    )
    
    # Batch insert Vector
    batch_size = 50
    for i in range(0, len(child_docs), batch_size):
        try:
            vector_store.add_documents(child_docs[i : i + batch_size])
            print(f"   -> Vector Batch {i} OK")
        except Exception as e:
            print(f"   âŒ Vector Batch {i} Lá»—i: {e}")
            time.sleep(2)

    print("ğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜!")

# --- MAIN ---
if __name__ == "__main__":
    init_db()
    
    # 1. Äá»c PDF
    raw_data = parse_pdf_data(INPUT_PDF_PATH, START_PAGE, END_PAGE)
    
    if raw_data:
        # 2. Format dá»¯ liá»‡u
        parents, children = prepare_documents(raw_data)
        
        # 3. LÆ°u
        save_to_db(parents, children)
    else:
        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u.")