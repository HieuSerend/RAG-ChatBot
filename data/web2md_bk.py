from curl_cffi import requests  # <--- DÃ™NG CÃI NÃ€Y THAY REQUESTS THÆ¯á»œNG
from bs4 import BeautifulSoup, NavigableString, Tag
import time
import re
import os
import random # <--- ThÃªm random Ä‘á»ƒ delay tá»± nhiÃªn
from dotenv import load_dotenv

# --- Cáº¤U HÃŒNH ---
load_dotenv(dotenv_path="env")

BASE = os.getenv("BASE_URL", "https://www.investopedia.com")
START_URL = os.getenv("START_URL", "https://www.investopedia.com/financial-term-dictionary-4769738")
DATA_DIR = os.getenv("DATA_DIR", "investopedia_terms")

# ============================================================
# 1) CLEAN FILE NAME
# ============================================================

def slugify(text: str):
    text = text.lower().strip()
    text = re.sub(r"[^a-z0-9]+", "-", text)
    return text.strip("-")

# ============================================================
# 2) HTML â†’ MARKDOWN (Giá»¯ nguyÃªn logic cá»§a báº¡n)
# ============================================================

def html_to_markdown(element):
    md = ""
    # LÆ°u Ã½: descendants duyá»‡t cáº£ con láº«n chÃ¡u, dá»… bá»‹ láº·p text.
    # MÃ¬nh thÃªm check parent Ä‘á»ƒ háº¡n cháº¿ láº·p, nhÆ°ng váº«n giá»¯ logic gá»‘c cá»§a báº¡n.
    for child in element.descendants:
        if isinstance(child, NavigableString):
            text = child.strip()
            # Chá»‰ láº¥y text náº¿u tháº» cha khÃ´ng náº±m trong danh sÃ¡ch tháº» block (trÃ¡nh láº·p)
            if text and child.parent.name not in ["p", "h1", "h2", "h3", "h4", "li", "strong", "em", "a", "td", "th"]:
                md += text + " "

        elif isinstance(child, Tag):
            if child.name in ["h1", "h2", "h3", "h4"]:
                level = int(child.name[1])
                md += "\n" + ("#" * level) + " " + child.get_text(strip=True) + "\n\n"
            elif child.name == "p":
                md += "\n" + child.get_text(strip=True) + "\n\n"
            elif child.name == "li":
                md += f"- {child.get_text(strip=True)}\n"
            elif child.name == "strong":
                md += f"**{child.get_text(strip=True)}** "
            elif child.name == "em":
                md += f"*{child.get_text(strip=True)}* "
            elif child.name == "a":
                href = child.get("href", "")
                txt = child.get_text(strip=True)
                if href.startswith("/"): href = BASE + href
                md += f"[{txt}]({href}) "
            elif child.name == "img":
                alt = child.get("alt", "")
                src = child.get("src", "")
                if src.startswith("/"): src = BASE + src
                md += f"![{alt}]({src})\n\n"
            elif child.name == "table":
                md += "\n\n" + html_table_to_md(child) + "\n\n"
    return md.strip()

def html_table_to_md(table_tag):
    rows = table_tag.find_all("tr")
    md = ""
    for i, row in enumerate(rows):
        cols = [c.get_text(strip=True) for c in row.find_all(["td", "th"])]
        if not cols: continue
        line = "| " + " | ".join(cols) + " |\n"
        md += line
        if i == 0: md += "| " + " | ".join(["---"] * len(cols)) + " |\n"
    return md

# ============================================================
# 3) Táº O FOLDER OUTPUT
# ============================================================

os.makedirs(DATA_DIR, exist_ok=True)

# ============================================================
# 4) Láº¤Y LIST TERM (ÄÃƒ Sá»¬A Lá»–I 403 VÃ€ SELECTOR)
# ============================================================

print("Äang táº£i danh sÃ¡ch term...")

try:
    # impersonate="chrome120" giáº£ láº­p trÃ¬nh duyá»‡t Chrome tháº­t
    # timeout=30 trÃ¡nh treo mÃ¡y
    response = requests.get(START_URL, impersonate="chrome120", timeout=30)
    
    if response.status_code != 200:
        print(f"Lá»–I: Server tráº£ vá» code {response.status_code}")
        exit()
        
    soup = BeautifulSoup(response.text, "html.parser")

    # --- Sá»¬A SELECTOR ---
    # Selector cÅ© cá»§a báº¡n: ".dictionary-top24-list__sublist.mntl-text-link" (sai, vÃ¬ nÃ³ tÃ¬m tháº» cÃ³ cáº£ 2 class cÃ¹ng lÃºc)
    # Selector má»›i: TÃ¬m tháº» 'a' náº±m BÃŠN TRONG tháº» list
    items = soup.select(".dictionary-top24-list__sublist a.mntl-text-link")
    
    # Fallback: Náº¿u web Ä‘á»•i cáº¥u trÃºc, thá»­ tÃ¬m tháº» a trong ID ná»™i dung
    if not items:
         items = soup.select("#dictionary-top24-list__sublist-content_1-0 a")

    links = []
    for item in items:
        name = item.get_text(strip=True)
        href = item.get("href")
        if href and href.startswith("/"):
            href = BASE + href
        links.append((name, href))

    print(f"âœ… ÄÃ£ láº¥y {len(links)} links.\n")

except Exception as e:
    print(f"âŒ Lá»—i káº¿t ná»‘i ban Ä‘áº§u: {e}")
    exit()

# ============================================================
# 5) SCRAPE Tá»ªNG TERM
# ============================================================

# Táº¡o session Ä‘á»ƒ giá»¯ káº¿t ná»‘i, giÃºp táº£i nhanh hÆ¡n vÃ  Ã­t bá»‹ cháº·n hÆ¡n
session = requests.Session()

for idx, (term, link) in enumerate(links, 1):
    slug = slugify(term)
    filename = f"{DATA_DIR}/{slug}.md"
    
    # Kiá»ƒm tra file tá»“n táº¡i Ä‘á»ƒ resume náº¿u bá»‹ ngáº¯t
    if os.path.exists(filename):
        print(f"[{idx}/{len(links)}] â­ï¸ ÄÃ£ cÃ³: {term}")
        continue

    print(f"[{idx}/{len(links)}] â¬‡ï¸ Äang táº£i: {term}")

    # Random delay (quan trá»ng Ä‘á»ƒ trÃ¡nh bá»‹ Cloudflare phÃ¡t hiá»‡n bot hÃ ng loáº¡t)
    time.sleep(random.uniform(2, 5))

    try:
        # DÃ¹ng session Ä‘á»ƒ táº£i trang con
        page = session.get(link, impersonate="chrome120", timeout=30)
        
        if page.status_code != 200:
            print(f"   âš ï¸ Lá»—i táº£i trang (Code {page.status_code})")
            continue

        s = BeautifulSoup(page.text, "html.parser")

        # Selector ná»™i dung (Update cho chuáº©n trang Investopedia hiá»‡n táº¡i)
        content = s.select_one("#mntl-sc-page_1-0") 
        if not content:
            content = s.select_one(".mntl-sc-page") # Class dá»± phÃ²ng

        if not content:
            print("   âš ï¸ KhÃ´ng tÃ¬m tháº¥y ná»™i dung bÃ i viáº¿t (HTML khÃ¡c máº«u)!")
            continue

        md = html_to_markdown(content)

        with open(filename, "w", encoding="utf-8") as f:
            f.write(f"# {term}\n\n")
            f.write(f"Source: {link}\n\n")
            f.write(md)

        print(f"   âœ… ÄÃ£ lÆ°u file.")

    except Exception as e:
        print(f"   âŒ Lá»—i ngoáº¡i lá»‡: {e}")

print("\nðŸŽ‰ DONE! Kiá»ƒm tra thÆ° má»¥c 'investopedia_terms/'")